{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "dense_network.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR0VPq1xBKgM"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "id": "dR0VPq1xBKgM",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRmdQl2ir1Xn"
      },
      "source": [
        "# Toy dataset"
      ],
      "id": "FRmdQl2ir1Xn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPUBxUtAR2cd",
        "outputId": "749410c2-cfed-4103-b498-9472343b1cd1"
      },
      "source": [
        "data = torch.tensor([[1, 0, 1, 0],\n",
        "                     [0, 0, 1, 0],\n",
        "                     [1, 1, 1, 1],\n",
        "                     [1, 1, 1, 0],\n",
        "                     [1, 0, 0, 0],\n",
        "                     [0, 0, 0, 0],\n",
        "                     [1, 1, 0, 1],\n",
        "                     [0, 1, 0, 0]\n",
        "                     ], dtype=torch.float)\n",
        "labels = torch.tensor([1, 0, 1, 1, 0, 0, 1, 0]).T\n",
        "\n",
        "labels"
      ],
      "id": "uPUBxUtAR2cd",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 0, 1, 1, 0, 0, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUNqtNIlsBrx"
      },
      "source": [
        "# Fully Connected Network"
      ],
      "id": "FUNqtNIlsBrx"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dried-tender"
      },
      "source": [
        "class DenseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DenseNetwork, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(4, 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4, 4),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4, 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2, 1),\n",
        "            nn.Sigmoid(),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.net(x)\n",
        "        return x"
      ],
      "id": "dried-tender",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTtewaUlW5Fp",
        "outputId": "495c98d3-6f3a-4642-8c27-50659697e33b"
      },
      "source": [
        "model = DenseNetwork()\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    print('cuda available')\n",
        "print(model)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(model(data.cuda()))\n",
        "else:\n",
        "    print(model(data))"
      ],
      "id": "BTtewaUlW5Fp",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda available\n",
            "DenseNetwork(\n",
            "  (net): Sequential(\n",
            "    (0): Linear(in_features=4, out_features=4, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=4, out_features=4, bias=True)\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=4, out_features=2, bias=True)\n",
            "    (6): ReLU()\n",
            "    (7): Linear(in_features=2, out_features=1, bias=True)\n",
            "    (8): Sigmoid()\n",
            "  )\n",
            ")\n",
            "tensor([[0.5650],\n",
            "        [0.5650],\n",
            "        [0.5650],\n",
            "        [0.5650],\n",
            "        [0.5650],\n",
            "        [0.5650],\n",
            "        [0.5650],\n",
            "        [0.5650]], device='cuda:0', grad_fn=<SigmoidBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QemNiM1sOAm"
      },
      "source": [
        "## Optimizer and Loss Criterion"
      ],
      "id": "_QemNiM1sOAm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPjhLcTDC1HA"
      },
      "source": [
        "# create a stochastic gradient descent optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "# create a loss function - binary cross-entropy\n",
        "criterion = nn.BCELoss()"
      ],
      "id": "uPjhLcTDC1HA",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7g5tnAssip_"
      },
      "source": [
        "## Data Loader"
      ],
      "id": "D7g5tnAssip_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nW6vNkyVfYUo",
        "outputId": "053a02da-dc65-40e8-8fe3-f517eb52c976"
      },
      "source": [
        "params = {'batch_size': 2,\n",
        "          'shuffle': True,\n",
        "          'num_workers': 2,\n",
        "          'pin_memory' : True}\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(data, labels), **params)\n",
        "for batch_idx, batch in enumerate(train_loader):\n",
        "    print(batch_idx, batch)"
      ],
      "id": "nW6vNkyVfYUo",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [tensor([[1., 1., 0., 1.],\n",
            "        [1., 0., 1., 0.]]), tensor([1, 1])]\n",
            "1 [tensor([[0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0.]]), tensor([0, 1])]\n",
            "2 [tensor([[0., 1., 0., 0.],\n",
            "        [0., 0., 1., 0.]]), tensor([0, 0])]\n",
            "3 [tensor([[1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1.]]), tensor([0, 1])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW3LM0SxsngZ"
      },
      "source": [
        "# Training"
      ],
      "id": "rW3LM0SxsngZ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixuDxq0_HuvH",
        "outputId": "d974ce7c-0e8b-4723-c907-7c0abd390cbb"
      },
      "source": [
        "log_interval = 100\n",
        "epochs = 200\n",
        "model.train()\n",
        "\n",
        "# training loop\n",
        "for epoch in range(epochs):\n",
        "    for batch_idx, (batch, target) in enumerate(train_loader):\n",
        "        if torch.cuda.is_available():\n",
        "            batch, target = batch.cuda(), target.cuda()\n",
        "        # resize data\n",
        "        #data = data.view(-1, 4)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch)\n",
        "        loss = criterion(output, target.unsqueeze(1).float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                    epoch,\n",
        "                    batch_idx * len(data),\n",
        "                    len(train_loader.dataset),\n",
        "                    100. * batch_idx / len(train_loader),\n",
        "                    loss.data))"
      ],
      "id": "ixuDxq0_HuvH",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 0 [0/8 (0%)]\tLoss: 0.811884\n",
            "Train Epoch: 1 [0/8 (0%)]\tLoss: 0.703495\n",
            "Train Epoch: 2 [0/8 (0%)]\tLoss: 0.615301\n",
            "Train Epoch: 3 [0/8 (0%)]\tLoss: 0.698886\n",
            "Train Epoch: 4 [0/8 (0%)]\tLoss: 0.693518\n",
            "Train Epoch: 5 [0/8 (0%)]\tLoss: 0.595297\n",
            "Train Epoch: 6 [0/8 (0%)]\tLoss: 0.859790\n",
            "Train Epoch: 7 [0/8 (0%)]\tLoss: 0.693300\n",
            "Train Epoch: 8 [0/8 (0%)]\tLoss: 0.693971\n",
            "Train Epoch: 9 [0/8 (0%)]\tLoss: 0.695116\n",
            "Train Epoch: 10 [0/8 (0%)]\tLoss: 0.693301\n",
            "Train Epoch: 11 [0/8 (0%)]\tLoss: 0.696422\n",
            "Train Epoch: 12 [0/8 (0%)]\tLoss: 0.680013\n",
            "Train Epoch: 13 [0/8 (0%)]\tLoss: 0.654817\n",
            "Train Epoch: 14 [0/8 (0%)]\tLoss: 0.693729\n",
            "Train Epoch: 15 [0/8 (0%)]\tLoss: 0.693979\n",
            "Train Epoch: 16 [0/8 (0%)]\tLoss: 0.801673\n",
            "Train Epoch: 17 [0/8 (0%)]\tLoss: 0.693208\n",
            "Train Epoch: 18 [0/8 (0%)]\tLoss: 0.690119\n",
            "Train Epoch: 19 [0/8 (0%)]\tLoss: 0.694779\n",
            "Train Epoch: 20 [0/8 (0%)]\tLoss: 0.693992\n",
            "Train Epoch: 21 [0/8 (0%)]\tLoss: 0.693340\n",
            "Train Epoch: 22 [0/8 (0%)]\tLoss: 0.779768\n",
            "Train Epoch: 23 [0/8 (0%)]\tLoss: 0.686201\n",
            "Train Epoch: 24 [0/8 (0%)]\tLoss: 0.694739\n",
            "Train Epoch: 25 [0/8 (0%)]\tLoss: 0.694568\n",
            "Train Epoch: 26 [0/8 (0%)]\tLoss: 0.693955\n",
            "Train Epoch: 27 [0/8 (0%)]\tLoss: 0.693371\n",
            "Train Epoch: 28 [0/8 (0%)]\tLoss: 0.713762\n",
            "Train Epoch: 29 [0/8 (0%)]\tLoss: 0.650738\n",
            "Train Epoch: 30 [0/8 (0%)]\tLoss: 0.693495\n",
            "Train Epoch: 31 [0/8 (0%)]\tLoss: 0.685936\n",
            "Train Epoch: 32 [0/8 (0%)]\tLoss: 0.761529\n",
            "Train Epoch: 33 [0/8 (0%)]\tLoss: 0.696778\n",
            "Train Epoch: 34 [0/8 (0%)]\tLoss: 0.786576\n",
            "Train Epoch: 35 [0/8 (0%)]\tLoss: 0.693555\n",
            "Train Epoch: 36 [0/8 (0%)]\tLoss: 0.660464\n",
            "Train Epoch: 37 [0/8 (0%)]\tLoss: 0.694090\n",
            "Train Epoch: 38 [0/8 (0%)]\tLoss: 0.693499\n",
            "Train Epoch: 39 [0/8 (0%)]\tLoss: 0.697355\n",
            "Train Epoch: 40 [0/8 (0%)]\tLoss: 0.694999\n",
            "Train Epoch: 41 [0/8 (0%)]\tLoss: 0.703856\n",
            "Train Epoch: 42 [0/8 (0%)]\tLoss: 0.696315\n",
            "Train Epoch: 43 [0/8 (0%)]\tLoss: 0.693298\n",
            "Train Epoch: 44 [0/8 (0%)]\tLoss: 0.693411\n",
            "Train Epoch: 45 [0/8 (0%)]\tLoss: 0.706586\n",
            "Train Epoch: 46 [0/8 (0%)]\tLoss: 0.694284\n",
            "Train Epoch: 47 [0/8 (0%)]\tLoss: 0.693832\n",
            "Train Epoch: 48 [0/8 (0%)]\tLoss: 0.694259\n",
            "Train Epoch: 49 [0/8 (0%)]\tLoss: 0.696320\n",
            "Train Epoch: 50 [0/8 (0%)]\tLoss: 0.695042\n",
            "Train Epoch: 51 [0/8 (0%)]\tLoss: 0.674744\n",
            "Train Epoch: 52 [0/8 (0%)]\tLoss: 0.693149\n",
            "Train Epoch: 53 [0/8 (0%)]\tLoss: 0.767667\n",
            "Train Epoch: 54 [0/8 (0%)]\tLoss: 0.665731\n",
            "Train Epoch: 55 [0/8 (0%)]\tLoss: 0.695684\n",
            "Train Epoch: 56 [0/8 (0%)]\tLoss: 0.658146\n",
            "Train Epoch: 57 [0/8 (0%)]\tLoss: 0.711614\n",
            "Train Epoch: 58 [0/8 (0%)]\tLoss: 0.694641\n",
            "Train Epoch: 59 [0/8 (0%)]\tLoss: 0.772093\n",
            "Train Epoch: 60 [0/8 (0%)]\tLoss: 0.693781\n",
            "Train Epoch: 61 [0/8 (0%)]\tLoss: 0.698984\n",
            "Train Epoch: 62 [0/8 (0%)]\tLoss: 0.683558\n",
            "Train Epoch: 63 [0/8 (0%)]\tLoss: 0.693186\n",
            "Train Epoch: 64 [0/8 (0%)]\tLoss: 0.675831\n",
            "Train Epoch: 65 [0/8 (0%)]\tLoss: 0.695199\n",
            "Train Epoch: 66 [0/8 (0%)]\tLoss: 0.693415\n",
            "Train Epoch: 67 [0/8 (0%)]\tLoss: 0.693797\n",
            "Train Epoch: 68 [0/8 (0%)]\tLoss: 0.690593\n",
            "Train Epoch: 69 [0/8 (0%)]\tLoss: 0.647306\n",
            "Train Epoch: 70 [0/8 (0%)]\tLoss: 0.695770\n",
            "Train Epoch: 71 [0/8 (0%)]\tLoss: 0.693154\n",
            "Train Epoch: 72 [0/8 (0%)]\tLoss: 0.693461\n",
            "Train Epoch: 73 [0/8 (0%)]\tLoss: 0.693152\n",
            "Train Epoch: 74 [0/8 (0%)]\tLoss: 0.673778\n",
            "Train Epoch: 75 [0/8 (0%)]\tLoss: 0.693966\n",
            "Train Epoch: 76 [0/8 (0%)]\tLoss: 0.696305\n",
            "Train Epoch: 77 [0/8 (0%)]\tLoss: 0.637553\n",
            "Train Epoch: 78 [0/8 (0%)]\tLoss: 0.760285\n",
            "Train Epoch: 79 [0/8 (0%)]\tLoss: 0.695470\n",
            "Train Epoch: 80 [0/8 (0%)]\tLoss: 0.729099\n",
            "Train Epoch: 81 [0/8 (0%)]\tLoss: 0.750911\n",
            "Train Epoch: 82 [0/8 (0%)]\tLoss: 0.693209\n",
            "Train Epoch: 83 [0/8 (0%)]\tLoss: 0.693152\n",
            "Train Epoch: 84 [0/8 (0%)]\tLoss: 0.689581\n",
            "Train Epoch: 85 [0/8 (0%)]\tLoss: 0.696838\n",
            "Train Epoch: 86 [0/8 (0%)]\tLoss: 0.694062\n",
            "Train Epoch: 87 [0/8 (0%)]\tLoss: 0.693839\n",
            "Train Epoch: 88 [0/8 (0%)]\tLoss: 0.694139\n",
            "Train Epoch: 89 [0/8 (0%)]\tLoss: 0.699908\n",
            "Train Epoch: 90 [0/8 (0%)]\tLoss: 0.696239\n",
            "Train Epoch: 91 [0/8 (0%)]\tLoss: 0.693213\n",
            "Train Epoch: 92 [0/8 (0%)]\tLoss: 0.791938\n",
            "Train Epoch: 93 [0/8 (0%)]\tLoss: 0.695425\n",
            "Train Epoch: 94 [0/8 (0%)]\tLoss: 0.694115\n",
            "Train Epoch: 95 [0/8 (0%)]\tLoss: 0.695340\n",
            "Train Epoch: 96 [0/8 (0%)]\tLoss: 0.693258\n",
            "Train Epoch: 97 [0/8 (0%)]\tLoss: 0.750630\n",
            "Train Epoch: 98 [0/8 (0%)]\tLoss: 0.698576\n",
            "Train Epoch: 99 [0/8 (0%)]\tLoss: 0.700924\n",
            "Train Epoch: 100 [0/8 (0%)]\tLoss: 0.755584\n",
            "Train Epoch: 101 [0/8 (0%)]\tLoss: 0.759939\n",
            "Train Epoch: 102 [0/8 (0%)]\tLoss: 0.693155\n",
            "Train Epoch: 103 [0/8 (0%)]\tLoss: 0.693342\n",
            "Train Epoch: 104 [0/8 (0%)]\tLoss: 0.686990\n",
            "Train Epoch: 105 [0/8 (0%)]\tLoss: 0.693208\n",
            "Train Epoch: 106 [0/8 (0%)]\tLoss: 0.693323\n",
            "Train Epoch: 107 [0/8 (0%)]\tLoss: 0.697061\n",
            "Train Epoch: 108 [0/8 (0%)]\tLoss: 0.689181\n",
            "Train Epoch: 109 [0/8 (0%)]\tLoss: 0.694693\n",
            "Train Epoch: 110 [0/8 (0%)]\tLoss: 0.718943\n",
            "Train Epoch: 111 [0/8 (0%)]\tLoss: 0.693232\n",
            "Train Epoch: 112 [0/8 (0%)]\tLoss: 0.613759\n",
            "Train Epoch: 113 [0/8 (0%)]\tLoss: 0.564640\n",
            "Train Epoch: 114 [0/8 (0%)]\tLoss: 0.797207\n",
            "Train Epoch: 115 [0/8 (0%)]\tLoss: 0.590913\n",
            "Train Epoch: 116 [0/8 (0%)]\tLoss: 0.709560\n",
            "Train Epoch: 117 [0/8 (0%)]\tLoss: 0.811318\n",
            "Train Epoch: 118 [0/8 (0%)]\tLoss: 0.578680\n",
            "Train Epoch: 119 [0/8 (0%)]\tLoss: 0.704005\n",
            "Train Epoch: 120 [0/8 (0%)]\tLoss: 0.693839\n",
            "Train Epoch: 121 [0/8 (0%)]\tLoss: 0.694590\n",
            "Train Epoch: 122 [0/8 (0%)]\tLoss: 0.694994\n",
            "Train Epoch: 123 [0/8 (0%)]\tLoss: 0.695248\n",
            "Train Epoch: 124 [0/8 (0%)]\tLoss: 0.716670\n",
            "Train Epoch: 125 [0/8 (0%)]\tLoss: 0.693814\n",
            "Train Epoch: 126 [0/8 (0%)]\tLoss: 0.662521\n",
            "Train Epoch: 127 [0/8 (0%)]\tLoss: 0.694411\n",
            "Train Epoch: 128 [0/8 (0%)]\tLoss: 0.693375\n",
            "Train Epoch: 129 [0/8 (0%)]\tLoss: 0.665745\n",
            "Train Epoch: 130 [0/8 (0%)]\tLoss: 0.795503\n",
            "Train Epoch: 131 [0/8 (0%)]\tLoss: 0.695608\n",
            "Train Epoch: 132 [0/8 (0%)]\tLoss: 0.694897\n",
            "Train Epoch: 133 [0/8 (0%)]\tLoss: 0.693154\n",
            "Train Epoch: 134 [0/8 (0%)]\tLoss: 0.693243\n",
            "Train Epoch: 135 [0/8 (0%)]\tLoss: 0.680222\n",
            "Train Epoch: 136 [0/8 (0%)]\tLoss: 0.693261\n",
            "Train Epoch: 137 [0/8 (0%)]\tLoss: 0.696162\n",
            "Train Epoch: 138 [0/8 (0%)]\tLoss: 0.693967\n",
            "Train Epoch: 139 [0/8 (0%)]\tLoss: 0.724295\n",
            "Train Epoch: 140 [0/8 (0%)]\tLoss: 0.678928\n",
            "Train Epoch: 141 [0/8 (0%)]\tLoss: 0.725845\n",
            "Train Epoch: 142 [0/8 (0%)]\tLoss: 0.693183\n",
            "Train Epoch: 143 [0/8 (0%)]\tLoss: 0.693668\n",
            "Train Epoch: 144 [0/8 (0%)]\tLoss: 0.693514\n",
            "Train Epoch: 145 [0/8 (0%)]\tLoss: 0.713859\n",
            "Train Epoch: 146 [0/8 (0%)]\tLoss: 0.677269\n",
            "Train Epoch: 147 [0/8 (0%)]\tLoss: 0.699343\n",
            "Train Epoch: 148 [0/8 (0%)]\tLoss: 0.718854\n",
            "Train Epoch: 149 [0/8 (0%)]\tLoss: 0.693152\n",
            "Train Epoch: 150 [0/8 (0%)]\tLoss: 0.730020\n",
            "Train Epoch: 151 [0/8 (0%)]\tLoss: 0.693750\n",
            "Train Epoch: 152 [0/8 (0%)]\tLoss: 0.693197\n",
            "Train Epoch: 153 [0/8 (0%)]\tLoss: 0.693999\n",
            "Train Epoch: 154 [0/8 (0%)]\tLoss: 0.696445\n",
            "Train Epoch: 155 [0/8 (0%)]\tLoss: 0.766399\n",
            "Train Epoch: 156 [0/8 (0%)]\tLoss: 0.741838\n",
            "Train Epoch: 157 [0/8 (0%)]\tLoss: 0.693148\n",
            "Train Epoch: 158 [0/8 (0%)]\tLoss: 0.644192\n",
            "Train Epoch: 159 [0/8 (0%)]\tLoss: 0.566637\n",
            "Train Epoch: 160 [0/8 (0%)]\tLoss: 0.693632\n",
            "Train Epoch: 161 [0/8 (0%)]\tLoss: 0.699396\n",
            "Train Epoch: 162 [0/8 (0%)]\tLoss: 0.700516\n",
            "Train Epoch: 163 [0/8 (0%)]\tLoss: 0.626815\n",
            "Train Epoch: 164 [0/8 (0%)]\tLoss: 0.616902\n",
            "Train Epoch: 165 [0/8 (0%)]\tLoss: 0.734034\n",
            "Train Epoch: 166 [0/8 (0%)]\tLoss: 0.712339\n",
            "Train Epoch: 167 [0/8 (0%)]\tLoss: 0.875457\n",
            "Train Epoch: 168 [0/8 (0%)]\tLoss: 0.695204\n",
            "Train Epoch: 169 [0/8 (0%)]\tLoss: 0.695683\n",
            "Train Epoch: 170 [0/8 (0%)]\tLoss: 0.693192\n",
            "Train Epoch: 171 [0/8 (0%)]\tLoss: 0.746775\n",
            "Train Epoch: 172 [0/8 (0%)]\tLoss: 0.716891\n",
            "Train Epoch: 173 [0/8 (0%)]\tLoss: 0.678364\n",
            "Train Epoch: 174 [0/8 (0%)]\tLoss: 0.694346\n",
            "Train Epoch: 175 [0/8 (0%)]\tLoss: 0.693362\n",
            "Train Epoch: 176 [0/8 (0%)]\tLoss: 0.629239\n",
            "Train Epoch: 177 [0/8 (0%)]\tLoss: 0.617557\n",
            "Train Epoch: 178 [0/8 (0%)]\tLoss: 0.693235\n",
            "Train Epoch: 179 [0/8 (0%)]\tLoss: 0.697661\n",
            "Train Epoch: 180 [0/8 (0%)]\tLoss: 0.606832\n",
            "Train Epoch: 181 [0/8 (0%)]\tLoss: 0.700571\n",
            "Train Epoch: 182 [0/8 (0%)]\tLoss: 0.717750\n",
            "Train Epoch: 183 [0/8 (0%)]\tLoss: 0.698175\n",
            "Train Epoch: 184 [0/8 (0%)]\tLoss: 0.698831\n",
            "Train Epoch: 185 [0/8 (0%)]\tLoss: 0.694889\n",
            "Train Epoch: 186 [0/8 (0%)]\tLoss: 0.691955\n",
            "Train Epoch: 187 [0/8 (0%)]\tLoss: 0.696912\n",
            "Train Epoch: 188 [0/8 (0%)]\tLoss: 0.650071\n",
            "Train Epoch: 189 [0/8 (0%)]\tLoss: 0.693254\n",
            "Train Epoch: 190 [0/8 (0%)]\tLoss: 0.733255\n",
            "Train Epoch: 191 [0/8 (0%)]\tLoss: 0.717361\n",
            "Train Epoch: 192 [0/8 (0%)]\tLoss: 0.699478\n",
            "Train Epoch: 193 [0/8 (0%)]\tLoss: 0.754864\n",
            "Train Epoch: 194 [0/8 (0%)]\tLoss: 0.694451\n",
            "Train Epoch: 195 [0/8 (0%)]\tLoss: 0.695494\n",
            "Train Epoch: 196 [0/8 (0%)]\tLoss: 0.677616\n",
            "Train Epoch: 197 [0/8 (0%)]\tLoss: 0.694559\n",
            "Train Epoch: 198 [0/8 (0%)]\tLoss: 0.693150\n",
            "Train Epoch: 199 [0/8 (0%)]\tLoss: 0.684887\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWbDIEkDuV4X"
      },
      "source": [
        "# Predict"
      ],
      "id": "yWbDIEkDuV4X"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtZZsmnkmplf",
        "outputId": "babb613a-219a-4da1-e567-8440ecd1f802"
      },
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    if torch.cuda.is_available():\n",
        "        output = model(data.cuda())\n",
        "    else:\n",
        "        output = model(data.cuda())\n",
        "\n",
        "output"
      ],
      "id": "HtZZsmnkmplf",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[7.8667e-01],\n",
              "        [6.7580e-04],\n",
              "        [7.8667e-01],\n",
              "        [7.8667e-01],\n",
              "        [2.9068e-04],\n",
              "        [1.6755e-07],\n",
              "        [7.8667e-01],\n",
              "        [1.5554e-05]], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    }
  ]
}