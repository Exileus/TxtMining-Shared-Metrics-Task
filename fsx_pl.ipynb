{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch related imports\n",
    "import torch\n",
    "from torch.nn import *\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# PyTorch Lightning related imports\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# metrics\n",
    "from torchmetrics.functional import mean_absolute_error, mean_squared_error\n",
    "from metrics.regression_metrics import t_kendalltau, t_pearson, t_spearman\n",
    "\n",
    "# fnet architecture & RAdam\n",
    "from fnet.model import FNet\n",
    "from radam.radam import RAdam\n",
    "\n",
    "# Custom datamodule\n",
    "from datamodules.tm_datamodule import TextMiningDataModule\n",
    "\n",
    "# weights and biases // not working // disabled\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# ModelCheckpoint fails. Tutorial outdated. \n",
    "# Leaving here for improvements in the future\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "srt = [\"source\", \"reference\", \"translation\"]\n",
    "language_pairs = [\n",
    "    \"cs-en\",\n",
    "    \"de-en\",\n",
    "    \"en-fi\",\n",
    "    \"en-zh\",\n",
    "    \"ru-en\",\n",
    "    \"zh-en\",\n",
    "]\n",
    "scores = {pair: pd.read_csv(f\"corpus/{pair}/scores.csv\") for pair in language_pairs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = \"de-en\"\n",
    "embedding_ref = torch.from_numpy(np.load(f\"corpus/{pair}/laser.reference_embeds.npy\")).float()\n",
    "embedding_src = torch.from_numpy(np.load(f\"corpus/{pair}/laser.source_embeds.npy\")).float()\n",
    "embedding_hyp = torch.from_numpy(np.load(f\"corpus/{pair}/laser.translation_embeds.npy\")).float()\n",
    "score = torch.tensor(scores[pair][\"z-score\"]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21704])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(monitor='val_loss', patience=5, verbose=False, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationPredictionLogger(Callback):\n",
    "    def __init__(self, val_samples, num_samples=32):\n",
    "        super().__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.val_imgs, self.val_score = val_samples\n",
    "\n",
    "    def on_validation_batch_end(\n",
    "        self, trainer, pl_module,\n",
    "    ):\n",
    "        val_imgs = self.val_imgs.to(device=pl_module.device)\n",
    "        val_score = self.val_score.to(device=pl_module.device)\n",
    "\n",
    "        predictions = pl_module(val_imgs)\n",
    "        trainer.logger.experiment.log(\n",
    "            {\n",
    "                \"examples\": [\n",
    "                    wandb.Image(x, caption=f\"Prediction:{p}, Score: {y}\")\n",
    "                    for x, p, y in zip(\n",
    "                        val_imgs[: self.num_samples],\n",
    "                        predictions[: self.num_samples],\n",
    "                        val_score[: self.num_samples],\n",
    "                    )\n",
    "                ]\n",
    "            }\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_1 = 64\n",
    "FEATURES_2 = FEATURES_1 * 2\n",
    "FEATURES_3 = FEATURES_2 * 2\n",
    "FEATURES_4 = FEATURES_3 * 2\n",
    "FEATURES_5 = FEATURES_4 * 2\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, input_shape, learning_rate=0.001):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.c1 = Conv2d(3, FEATURES_1, (3, 1), 1, (1, 0))\n",
    "        self.c2 = Conv2d(FEATURES_1, FEATURES_1, (3, 1), 1, (1, 0)) # 32\n",
    "        self.c3 = Conv2d(FEATURES_1, FEATURES_2, (3, 1), 4, (1, 0)) # 24\n",
    "        self.c4 = Conv2d(FEATURES_2, FEATURES_3, (3, 1), 2, (1, 0)) # 16\n",
    "        self.c5 = Conv2d(FEATURES_3, FEATURES_4, (3, 1), 2, (1, 0)) # 8\n",
    "        self.c6 = Conv2d(FEATURES_4, FEATURES_5, (3, 1), 2, (1, 0)) # 2\n",
    "        self.bn = BatchNorm2d(FEATURES_1)\n",
    "        self.bn3 = BatchNorm2d(FEATURES_2)\n",
    "        self.bn4 = BatchNorm2d(FEATURES_3)\n",
    "        self.bn5 = BatchNorm2d(FEATURES_4)\n",
    "        self.bn6 = BatchNorm2d(FEATURES_5)\n",
    "        self.fc1 = Linear(32 * FEATURES_5, 256)\n",
    "        self.fc2 = Linear(256, 64)\n",
    "        self.fc3 = Linear(64, 1)\n",
    "\n",
    "    def _forward_features(self, x):\n",
    "        x = self.bn(F.relu(self.c1(x)))\n",
    "        # x = self.bn(F.relu(self.c2(x) + x))\n",
    "        # x = self.bn(F.relu(self.c2(x) + x))\n",
    "        # x = self.bn(F.relu(self.c2(x) + x))\n",
    "        x = self.bn(F.relu(self.c2(x)))\n",
    "        x = self.bn3(F.relu(self.c3(x)))\n",
    "        x = self.bn4(F.relu(self.c4(x)))\n",
    "        x = self.bn5(F.relu(self.c5(x)))\n",
    "        # x = self.bn6(F.relu(self.c6(x)))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self._forward_features(x)\n",
    "        # print(x.shape)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.mse_loss(logits, y)\n",
    "        mse = mean_squared_error(logits, y)\n",
    "        mae = mean_absolute_error(logits, y)\n",
    "        k = t_kendalltau(logits, y)\n",
    "        p = t_pearson(logits, y)\n",
    "        s = t_spearman(logits, y)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, logger=True)\n",
    "        self.log(\"train_mse\", mse, on_step=True, on_epoch=True, logger=True)\n",
    "        self.log(\"train_mae\", mae, on_step=True, on_epoch=True, logger=True)\n",
    "        self.log(\"train_kendalltau\", k, on_step=False, on_epoch=True, logger=True)\n",
    "        self.log(\"train_pearson\", p, on_step=False, on_epoch=True, logger=True)\n",
    "        self.log(\"train_spearman\", s, on_step=False, on_epoch=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.mse_loss(logits, y)\n",
    "        mse = mean_squared_error(logits, y)\n",
    "        mae = mean_absolute_error(logits, y)\n",
    "        k = t_kendalltau(logits, y)\n",
    "        p = t_pearson(logits, y)\n",
    "        s = t_spearman(logits, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_mse\", mse, prog_bar=True)\n",
    "        self.log(\"val_mae\", mae, prog_bar=True)\n",
    "        self.log(\"val_kendalltau\", k, on_step=False, prog_bar=True)\n",
    "        self.log(\"val_pearson\", p, on_step=False, prog_bar=True)\n",
    "        self.log(\"val_spearman\", s, on_step=False, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.mse_loss(logits, y)\n",
    "        mse = mean_squared_error(logits, y)\n",
    "        mae = mean_absolute_error(logits, y)\n",
    "        k = t_kendalltau(logits, y)\n",
    "        p = t_pearson(logits, y)\n",
    "        s = t_spearman(logits, y)\n",
    "        self.log(\"test_loss\", loss, on_step=True, prog_bar=True)\n",
    "        self.log(\"test_mse\", mse, on_step=True, prog_bar=True)\n",
    "        self.log(\"test_mae\", mae, on_step=True, prog_bar=True)\n",
    "        self.log(\"test_kendalltau\", k, prog_bar=True)\n",
    "        self.log(\"test_pearson\", p, prog_bar=True)\n",
    "        self.log(\"test_spearman\", s, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FourierTransformerDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape,\n",
    "        learning_rate=0.001,\n",
    "        num_layers: int = 6,\n",
    "        dropout: float = 0.1,\n",
    "        dim_ff:int = 2048\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decoder = FNet(num_layers, input_shape, dim_ff, dropout)\n",
    "        self.fc_bloc = Sequential(\n",
    "            Linear(input_shape, input_shape // (div := 2)),  # 1024 > 512\n",
    "            GELU(),\n",
    "            Dropout(dropout),\n",
    "            Linear(input_shape // div, input_shape // (div := div * 4)),  # 512 > 128\n",
    "            GELU(),\n",
    "            Dropout(dropout),\n",
    "            Linear(input_shape // div, 1),  # 128 > 1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        x = self.fc_bloc(x)\n",
    "        x = torch.tanh(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.mse_loss(logits, y)\n",
    "        mse = mean_squared_error(logits, y)\n",
    "        mae = mean_absolute_error(logits, y)\n",
    "        k = t_kendalltau(logits, y)\n",
    "        p = t_pearson(logits, y)\n",
    "        s = t_spearman(logits, y)\n",
    "\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, logger=True)\n",
    "        self.log(\"train_mse\", mse, on_step=True, on_epoch=True, logger=True)\n",
    "        self.log(\"train_mae\", mae, on_step=True, on_epoch=True, logger=True)\n",
    "        self.log(\"train_kendalltau\", k, on_step=False, on_epoch=True, logger=True)\n",
    "        self.log(\"train_pearson\", p, on_step=False, on_epoch=True, logger=True)\n",
    "        self.log(\"train_spearman\", s, on_step=False, on_epoch=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.mse_loss(logits, y)\n",
    "        mse = mean_squared_error(logits, y)\n",
    "        mae = mean_absolute_error(logits, y)\n",
    "        k = t_kendalltau(logits, y)\n",
    "        p = t_pearson(logits, y)\n",
    "        s = t_spearman(logits, y)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_mse\", mse, prog_bar=True)\n",
    "        self.log(\"val_mae\", mae, prog_bar=True)\n",
    "        self.log(\"val_kendalltau\", k, on_step=False, prog_bar=True)\n",
    "        self.log(\"val_pearson\", p, on_step=False, prog_bar=True)\n",
    "        self.log(\"val_spearman\", s, on_step=False, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.mse_loss(logits, y)\n",
    "        mse = mean_squared_error(logits, y)\n",
    "        mae = mean_absolute_error(logits, y)\n",
    "        k = t_kendalltau(logits, y)\n",
    "        p = t_pearson(logits, y)\n",
    "        s = t_spearman(logits, y)\n",
    "        self.log(\"test_loss\", loss, on_step=True, prog_bar=True)\n",
    "        self.log(\"test_mse\", mse, on_step=True, prog_bar=True)\n",
    "        self.log(\"test_mae\", mae, on_step=True, prog_bar=True)\n",
    "        self.log(\"test_kendalltau\", k, prog_bar=True)\n",
    "        self.log(\"test_pearson\", p, prog_bar=True)\n",
    "        self.log(\"test_spearman\", s, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        optimizer = RAdam(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training + Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = TextMiningDataModule(256, \"de-en\", 1024*3)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_samples = next(iter(dm.train_dataloader()))\n",
    "# val_imgs, val_score = val_samples\n",
    "# val_imgs.shape, val_score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "model = Model(dm.dims, learning_rate=0.001, dim_ff=4096)\n",
    "# wandb_logger = WandbLogger(project=\"wandb-lightning\", job_type=\"train\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    progress_bar_refresh_rate=1,\n",
    "    gpus=1,\n",
    "    # logger=wandb_logger,\n",
    "    # callbacks=[early_stop_callback],\n",
    ")\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | decoder | FNet       | 151 M \n",
      "1 | fc_bloc | Sequential | 5.3 M \n",
      "---------------------------------------\n",
      "156 M     Trainable params\n",
      "0         Non-trainable params\n",
      "156 M     Total params\n",
      "625.837   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 1/86 [00:00<00:56,  1.52it/s, loss=0.581, v_num=2, val_loss=0.691, val_mse=0.691, val_mae=0.684, val_kendalltau=-.0528, val_pearson=-.0384, val_spearman=-.0789]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fsx/nova/spring_21/TM/tm_project/radam/radam.py:58: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  /opt/conda/conda-bld/pytorch_1616554798336/work/torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 86/86 [00:16<00:00,  5.07it/s, loss=0.742, v_num=2, val_loss=0.728, val_mse=0.728, val_mae=0.690, val_kendalltau=0.095, val_pearson=0.148, val_spearman=0.141]\n"
     ]
    }
   ],
   "source": [
    "# torch.autograd.set_detect_anomaly(False)\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of radam.radam failed: Traceback (most recent call last):\n",
      "  File \"/home/fsx/miniconda3/envs/pl/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/fsx/miniconda3/envs/pl/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/fsx/miniconda3/envs/pl/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/fsx/miniconda3/envs/pl/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"/home/fsx/miniconda3/envs/pl/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/fsx/miniconda3/envs/pl/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 266, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: step() requires a code object with 1 free vars, not 0\n",
      "]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 VALIDATE RESULTS\n",
      "{'val_kendalltau': 0.09496324509382248,\n",
      " 'val_loss': 0.7278435826301575,\n",
      " 'val_mae': 0.6904030442237854,\n",
      " 'val_mse': 0.7278435826301575,\n",
      " 'val_pearson': 0.14763259887695312,\n",
      " 'val_spearman': 0.14055135846138}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.7278435826301575,\n",
       "  'val_mse': 0.7278435826301575,\n",
       "  'val_mae': 0.6904030442237854,\n",
       "  'val_kendalltau': 0.09496324509382248,\n",
       "  'val_pearson': 0.14763259887695312,\n",
       "  'val_spearman': 0.14055135846138}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8b00cba333cbc52f2b9024e2fd0f03f267bd4c88e752638d58a81ffee90e795"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('pl': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}