{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e53215c4",
   "metadata": {},
   "source": [
    "# Text Mining Grouo Project\n",
    "## DE-EN Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc50e48",
   "metadata": {},
   "source": [
    "##### TOC for Implemented Metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3170d863",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15c6d2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Installs.\n",
    "# !pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "119a6db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from rouge import Rouge\n",
    "from nltk.translate import chrf_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cacd29d",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e093ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5299d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21704 entries, 0 to 21703\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   source       21704 non-null  object \n",
      " 1   reference    21704 non-null  object \n",
      " 2   translation  21704 non-null  object \n",
      " 3   z-score      21704 non-null  float64\n",
      " 4   avg-score    21704 non-null  float64\n",
      " 5   annotators   21704 non-null  int64  \n",
      "dtypes: float64(2), int64(1), object(3)\n",
      "memory usage: 1017.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6946082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "      <th>z-score</th>\n",
       "      <th>avg-score</th>\n",
       "      <th>annotators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ihr Zeitlupentempo maßen sie, als sie vor Spit...</td>\n",
       "      <td>Her timeless pace measures them when they equi...</td>\n",
       "      <td>Their slow speed was measured by researchers o...</td>\n",
       "      <td>-0.345024</td>\n",
       "      <td>76.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Er sagte, dass die Bereiche ruhige Treffpunkte...</td>\n",
       "      <td>He said the areas offer quiet meeting points b...</td>\n",
       "      <td>He said the spaces provided calm meeting point...</td>\n",
       "      <td>0.903800</td>\n",
       "      <td>97.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Für die Geschäftsleute an der B 27 ist es nur ...</td>\n",
       "      <td>For businessmen at the B 27, it's only a small...</td>\n",
       "      <td>This is only a small consolation for businesse...</td>\n",
       "      <td>0.700503</td>\n",
       "      <td>94.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Diese Fähigkeit sei möglicherweise angeboren o...</td>\n",
       "      <td>This ability may be born or developed with gen...</td>\n",
       "      <td>This ability may be innate, or may develop as ...</td>\n",
       "      <td>-1.256572</td>\n",
       "      <td>51.5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Weil sie Wassertemperaturen um die sechs Grad ...</td>\n",
       "      <td>Because they prefer water temperatures around ...</td>\n",
       "      <td>They generally only come to the surface in win...</td>\n",
       "      <td>0.293909</td>\n",
       "      <td>87.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  Ihr Zeitlupentempo maßen sie, als sie vor Spit...   \n",
       "1  Er sagte, dass die Bereiche ruhige Treffpunkte...   \n",
       "2  Für die Geschäftsleute an der B 27 ist es nur ...   \n",
       "3  Diese Fähigkeit sei möglicherweise angeboren o...   \n",
       "4  Weil sie Wassertemperaturen um die sechs Grad ...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Her timeless pace measures them when they equi...   \n",
       "1  He said the areas offer quiet meeting points b...   \n",
       "2  For businessmen at the B 27, it's only a small...   \n",
       "3  This ability may be born or developed with gen...   \n",
       "4  Because they prefer water temperatures around ...   \n",
       "\n",
       "                                         translation   z-score  avg-score  \\\n",
       "0  Their slow speed was measured by researchers o... -0.345024       76.0   \n",
       "1  He said the spaces provided calm meeting point...  0.903800       97.5   \n",
       "2  This is only a small consolation for businesse...  0.700503       94.0   \n",
       "3  This ability may be innate, or may develop as ... -1.256572       51.5   \n",
       "4  They generally only come to the surface in win...  0.293909       87.0   \n",
       "\n",
       "   annotators  \n",
       "0           1  \n",
       "1           2  \n",
       "2           1  \n",
       "3           2  \n",
       "4           2  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f750bb",
   "metadata": {},
   "source": [
    "---\n",
    "### PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5f5b1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre processing of this set.\n",
    "df = df1.copy()\n",
    "\n",
    "for x in [\"source\",\"reference\",\"translation\"]:\n",
    "    # lowercase.\n",
    "    df[x] = df1[x].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047c0501",
   "metadata": {},
   "source": [
    "---\n",
    "### Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a0f358",
   "metadata": {},
   "source": [
    "Rouge metric as described in\n",
    "\n",
    "https://towardsdatascience.com/the-ultimate-performance-metric-in-nlp-111df6c64460"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ada24e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = df[\"reference\"][0]\n",
    "model_out = df[\"translation\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b834949",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13f31666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "her timeless pace measures them when they equipped six animals with a broadcaster before spitsbergen.\n",
      "their slow speed was measured by researchers off svalbard, who fitted six animals with a tracker.\n"
     ]
    }
   ],
   "source": [
    "print(reference)\n",
    "print(model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "548ad9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge-1': {'f': 0.2580645111342353, 'p': 0.25, 'r': 0.26666666666666666},\n",
       "  'rouge-2': {'f': 0.20689654673008337, 'p': 0.2, 'r': 0.21428571428571427},\n",
       "  'rouge-l': {'f': 0.2580645111342353, 'p': 0.25, 'r': 0.26666666666666666}}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The get scores method returns three metrics, F1 score, p precision and recall r.\n",
    "# For each unigram,bigram and Longest sequence.\n",
    "rouge.get_scores(model_out,reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faf1bcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the entire model, model_out and reference need to be lists of strings.\n",
    "model_out = df[\"translation\"].to_list()\n",
    "reference = df[\"reference\"].to_list()\n",
    "rouge_scores = rouge.get_scores(model_out,reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3728d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each of the three scores, output a new column in the df with the f1 scores.\n",
    "for key in rouge_scores[0].keys():\n",
    "    df[(key+\" score\")] = pd.Series([score[key][\"f\"] for score in rouge_scores])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57ab0b9",
   "metadata": {},
   "source": [
    "---\n",
    "#### chrF metric\n",
    "\n",
    "Check the paper here: https://www.aclweb.org/anthology/W15-3049.pdf\n",
    "\n",
    "The general formula for the CHRF score is:\n",
    "\n",
    "`CHRFBeta = (1 + Beta**2) * ((chrP * chrR) / (Beta**2*chrP + chrR))`\n",
    "\n",
    "where:\n",
    "* chrP is the percentage of n-grams in the hypothesis which have a counterpart in the reference.\n",
    "* chrR is the percentage of character n-grams in the reference which are also present in the hypothesis.\n",
    "* Beta is a parameter which assigns beta times more importance to recall than to precision (if beta == 1, they have the same importance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966dd29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I was surprised, but this works exactly like it's intended. Makes a new column with the chrF score for each row of the df.\n",
    "# The default n-gram values are min == 1, max == 6. \n",
    "# The default beta is 3.\n",
    "\n",
    "# All parameters to test chrf scores with. feel free to play around with this and test out different combinations.\n",
    "# Note: this takes a few minutes to run.\n",
    "min_len = [1,2]\n",
    "max_len = [6]\n",
    "beta = [1,3]\n",
    "\n",
    "chrf_scores = []\n",
    "for min_l in min_len:\n",
    "    for max_l in max_len:\n",
    "        for b in beta:\n",
    "            append_str = \"chrf_b\" + str(b) + \"_n\" + str(min_l) + str(max_l)\n",
    "            chrf_scores.append(append_str)\n",
    "            df[append_str] = df.apply(lambda row: chrf_score.sentence_chrf(row[\"reference\"],row[\"translation\"],min_len=min_l,max_len=max_l,beta=b),axis=1)\n",
    "\n",
    "df.loc[:,chrf_scores]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec48ad25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed23b6f2",
   "metadata": {},
   "source": [
    "---\n",
    "### Comparison of Applied Metrics\n",
    "Because the numeric system used for all of these can be different, the best way to compare them is by checking the correlation with the annotator's scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b20ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dict to be transformed to a df later, for score comparison.\n",
    "scores_dict = {\"pearson\":[],\"kendall\":[],\"spearman\":[]}\n",
    "scores_index = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac7e89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thankfully, Pandas has a corr method.\n",
    "\n",
    "# for each declared corr method, compute the corr between each computed metric and the avg-score column.\n",
    "for corr in scores_dict.keys():\n",
    "    for key in rouge_scores[0].keys():\n",
    "        scores_dict[corr].append(df.loc[:,(key+ \" score\")].corr(df.loc[:,\"avg-score\"],method=corr))\n",
    "\n",
    "for corr in scores_dict.keys():\n",
    "    for chrf_score in chrf_scores:\n",
    "        scores_dict[corr].append(df.loc[:,chrf_score].corr(df.loc[:,\"avg-score\"],method=corr))\n",
    "\n",
    "# Build also a list that will be used to create the index for the scores dataframe.\n",
    "scores_index.extend(list(rouge_scores[0].keys()))\n",
    "scores_index.extend(chrf_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6c3395",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = pd.DataFrame(scores_dict,index=scores_index)\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1776374",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
